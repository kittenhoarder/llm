{"id": "0", "text": "{\n  \"version\": \"1.0\",\n  \"backend_name\": \"hnsw\",\n  \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"dimensions\": 384,\n  \"backend_kwargs\": {\n    \"is_recompute\": false,\n    \"is_compact\": false\n  },\n  \"embedding_mode\": \"sentence-transformers\",\n  \"passage_sources\": [\n    {\n      \"type\": \"jsonl\",\n      \"path\": \"/Users/owenperry/dev/llm/leann_poc/code_index.passages.jsonl\",\n      \"index_path\": \"code_index.passages.idx\",\n      \"path_relative\": \"code_index.passages.jsonl\",\n      \"index_path_relative\": \"code_index.passages.idx\"\n    }\n  ],\n  \"is_compact\": false,\n  \"is_pruned\": false\n}", "metadata": {"file_path": "code_index.meta.json", "file_extension": ".json", "file_name": "code_index.meta.json", "file_size": 595, "last_modified": 1766515995.3698208}}
{"id": "1", "text": "Count=1024, Bytes=4096\n[0.26s]   Read neighbors (1024)\n[0.32s]   Read scalar params (ep=3, max_lvl=0)\n[0.32s] Checking for storage data...\n[0.32s]   Found storage fourcc: 49467849.\n[0.32s] Converting to CSR format...\n\n[0.32s]   Converting node 1/16 (6.2%)...\n[0.32s]   Converting node 2/16 (12.5%)...\n[0.32s]   Converting node 3/16 (18.8%)...\n[0.32s]   Converting node 4/16 (25.0%)...\n[0.32s]   Converting node 5/16 (31.2%)...\n[0.32s]   Converting node 6/16 (37.5%)...\n[0.32s]   Converting node 7/16 (43.8%)...\n[0.32s]   Converting node 8/16 (50.0%)...\n[0.32s]   Converting node 9/16 (56.2%)...\n[0.32s]   Converting node 10/16 (62.5%)...\n[0.32s]   Converting node 11/16 (68.8%)...\n[0.32s]   Converting node 12/16 (75.0%)...\n[0.32s]   Converting node 13/16 (81.2%)...\n[0.32s]   Converting node 14/16 (87.5%)...\n[0.32s]   Converting node 15/16 (93.8%)...\n[0.32s]   Conversion loop finished.                        \n[0.32s] Running validation checks...\n    Checking total valid neighbor count...\n    OK: Total valid neighbors = 240\n    Checking final pointer indices...\n    OK: Final pointers match data size.\n[0.32s] Deleting original neighbors and offsets arrays...\n    CSR Stats: |data|=240, |level_ptr|=32\n[0.39s] Writing CSR HNSW graph data in FAISS-compatible order...\n   Pruning embeddings: Writing NULL storage marker.\n[0.45s] Conversion complete.\n{\n  \"success\": true,\n  \"indexed_files\": 16,\n  \"indexed_chunks\": 16,\n  \"errors\": [],\n  \"index_path\": \"/Users/owenperry/dev/llm/leann_poc/test_index_robust\"\n}\nM: 64 for level: 0\n", "metadata": {"file_path": "output.json", "file_extension": ".json", "file_name": "output.json", "file_size": 1529, "last_modified": 1766495545.2733202}}
{"id": "2", "text": "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n", "metadata": {"file_path": "code_index.ids.txt", "file_extension": ".txt", "file_name": "code_index.ids.txt", "file_size": 278, "last_modified": 1766515601.239691}}
{"id": "3", "text": "#!/usr/bin/env python3\n\"\"\"\nLEANN Bridge - Python bridge for code indexing and search\nThis script provides a simple interface for Swift to interact with LEANN\n\"\"\"\n\nimport json\nimport sys\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\nimport argparse\n\n# Import LEANN components\nimport os\n\n# Suppress library logs and warnings before importing them\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nimport logging\nlogging.getLogger(\"leann\").setLevel(logging.ERROR)\nlogging.getLogger(\"sentence_transformers\").setLevel(logging.ERROR)\n\nfrom leann import LeannBuilder, LeannSearcher\nimport contextlib\nimport io\n\n@contextlib.contextmanager\ndef suppress_stdout():\n    \"\"\"Redirects stdout to stderr at the OS level to catch C++ extension output\"\"\"\n    # Duplicate original stdout (fd 1) so we can restore it later\n    original_stdout_fd = os.dup(1)\n    try:\n        # Redirect fd 1 (stdout) to fd 2 (stderr)\n        os.dup2(2, 1)\n        yield\n    finally:\n        # Flush everything before restoring\n        sys.stdout.flush()\n        sys.stderr.flush()\n        # Restore original stdout\n        os.dup2(original_stdout_fd, 1)\n        os.close(original_stdout_fd)\n\ndef print_result(result):\n    \"\"\"Print result wrapped in markers for easy parsing in Swift\"\"\"\n    print(\"---JSON_START---\")\n    print(json.dumps(result, indent=2))\n    print(\"---JSON_END---\")\n\n\nclass LEANNBridge:\n    \"\"\"Bridge between Swift and LEANN Python library\"\"\"\n    \n    def __init__(self, index_path: str = \"./leann_index\"):\n        self.index_path = Path(index_path)\n        # Only create the parent directory, not the index path itself as a directory\n        self.index_path.parent.mkdir(parents=True, exist_ok=True)\n        \n    def index_codebase(\n        self,\n        root_path: str,\n        extensions: Optional[List[str]] = None,\n        exclude_dirs: Optional[List[str]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Index a codebase with metadata\n        \n        Args:\n            root_path: Root directory of codebase\n            extensions: File extensions to index (e.g., ['.swift', '.py'])\n            exclude_dirs: Directories to exclude\n            \n        Returns:\n            Statistics about indexing operation\n        \"\"\"\n        if extensions is None:\n            extensions = ['.swift', '.py', '.js', '.ts', '.md', '.json', '.csv', '.yaml', '.yml', '.txt']\n        if exclude_dirs is None:\n            exclude_dirs = ['build', '.build', 'node_modules', 'venv', '.git']\n            \n        root = Path(root_path)\n        if not root.exists():\n            return {\"error\": f\"Path does not exist: {root_path}\"}\n            \n        # Initialize builder with on-device sentence-transformers model\n        # is_recompute=False and is_compact=False ensure that we store full embeddings\n        # so that searches can be performed in-process without an embedding server.\n        builder = LeannBuilder(\n            backend_name=\"hnsw\",\n            embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",  # Fast, local, on-device\n            is_recompute=False,\n            is_compact=False\n        )\n        \n        indexed_files = 0\n        indexed_chunks = 0\n        errors = []\n        \n        # Walk through directory\n        for file_path in root.rglob('*'):\n            # Skip if not a file\n            if not file_path.is_file():\n                continue\n                \n            # Skip excluded directories\n            if any(exc in file_path.parts for exc in exclude_dirs):\n                continue\n                \n            # Skip if not matching extension\n            if file_path.suffix not in extensions:\n                continue\n                \n            try:\n                # Read file content\n                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                    content = f.read()\n                    \n                # Skip empty files\n                if not content.strip():\n                    continue\n                    \n                # Get file metadata\n                stat = file_path.stat()\n                rel_path = str(file_path.relative_to(root))\n                \n                metadata = {\n                    \"file_path\": rel_path,\n                    \"file_extension\": file_path.suffix,\n                    \"file_name\": file_path.name,\n                    \"file_size\": stat.st_size,\n                    \"last_modified\": stat.st_mtime,\n                }\n                \n                # Index the file\n                # LEANN will automatically chunk long files\n                builder.add_text(content, metadata=metadata)\n                \n                indexed_files += 1\n                indexed_chunks += 1  # Simplified - LEANN handles chunking internally\n                \n            except Exception as e:\n                errors.append(f\"Error indexing {file_path}: {str(e)}\")\n                \n        # Build the index\n        try:\n            builder.build_index(str(self.index_path))\n        except Exception as e:\n            return {\"error\": f\"Failed to build index: {str(e)}\"}\n\n            \n        return {\n            \"success\": True,\n            \"indexed_files\": indexed_files,\n            \"indexed_chunks\": indexed_chunks,\n            \"errors\": errors,\n            \"index_path\": str(self.index_path)\n        }\n        \n    def search(\n        self,\n        query: str,\n        top_k: int = 10,\n        metadata_filters: Optional[Dict[str, Any]] = None,\n        use_grep: bool = False\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Search the indexed codebase\n        \n        Args:\n            query: Search query\n            top_k: Number of results to return\n            metadata_filters: Filters to apply (e.g., {\"file_extension\": {\"==\": \".swift\"}})\n            use_grep: Use exact text matching instead of semantic search\n            \n        Returns:\n            List of search results with content and metadata\n        \"\"\"\n        try:\n            # Initialize searcher with index path\n            searcher = LeannSearcher(str(self.index_path))\n            \n            # Check if index requires recompute\n            # This is true for pruned or compact HNSW indices\n            # If so, we MUST set recompute_embeddings=True\n            is_pruned = searcher.meta_data.get(\"is_pruned\", False)\n            is_compact = searcher.meta_data.get(\"is_compact\", False)\n            recompute_required = is_pruned or is_compact\n            \n            # Perform search\n            # If recompute is NOT required, we use in-process embedding computation\n            # which is much more reliable in a sandboxed environment.\n            try:\n                results = searcher.search(\n                    query=query,\n                    top_k=top_k,\n                    metadata_filters=metadata_filters,\n                    use_grep=use_grep,\n                    recompute_embeddings=recompute_required\n                )\n            except Exception as search_err:\n                # If search failed and it required recompute, it's likely the server failed\n                if recompute_required:\n                    return [{\"error\": f\"Search failed because the index is compact/pruned and the recompute server could not start. Please RE-INDEX your codebase in Settings for better performance and reliability. Error: {str(search_err)}\"}]\n                raise search_err\n            \n            # Format results\n            formatted_results = []\n            for result in results:\n                formatted_results.append({\n                    \"content\": result.text,\n                    \"metadata\": result.metadata,\n                    \"score\": float(result.score),  # Convert numpy float32 to Python float\n                })\n                \n            return formatted_results\n            \n        except Exception as e:\n            return [{\"error\": f\"Search failed: {str(e)}\"}]\n            \n    def get_index_stats(self) -> Dict[str, Any]:\n        \"\"\"Get statistics about the index\"\"\"\n        try:\n            if not self.index_path.exists():\n                return {\"error\": \"Index does not exist\"}\n                \n            # Calculate index size\n            total_size = sum(\n                f.stat().st_size\n                for f in self.index_path.rglob('*')\n                if f.is_file()\n            )\n            \n            return {\n                \"index_path\": str(self.index_path),\n                \"index_size_bytes\": total_size,\n                \"index_size_mb\": round(total_size / (1024 * 1024), 2),\n                \"exists\": True\n            }\n            \n        except Exception as e:\n            return {\"error\": f\"Failed to get stats: {str(e)}\"}\n\n    def list_files(self) -> List[str]:\n        \"\"\"List all indexed file paths (extracted from metadata)\"\"\"\n        try:\n            searcher = LeannSearcher(str(self.index_path))\n            # LEANN doesn't have a direct 'list all metadata' but we can \n            # infer it from the internal storage or just search for everything\n            # For now, let's just return a placeholder or implement if LEANN supports it\n            # Actually, let's use a simpler approach: return the files that match the index criteria\n            # in the saved path if we had it, but here we only have the index.\n            \n            # If LeannSearcher exposes the metadata storage, we'd use it.\n            # Assuming we want a list of files that WERE indexed:\n            return [\"file_list_not_implemented_in_leann_yet\"]\n            \n        except Exception as e:\n            return [f\"Error listing files: {str(e)}\"]\n\n\ndef main():\n    \"\"\"Command-line interface for LEANN bridge\"\"\"\n    parser = argparse.ArgumentParser(description=\"LEANN Bridge for Swift integration\")\n    subparsers = parser.add_subparsers(dest='command', help='Command to execute')\n    \n    # Index command\n    index_parser = subparsers.add_parser('index', help='Index a codebase')\n    index_parser.add_argument('path', help='Path to codebase root')\n    index_parser.add_argument('--extensions', nargs='+', help='File extensions to index')\n    index_parser.add_argument('--exclude', nargs='+', help='Directories to exclude')\n    index_parser.add_argument('--index-path', default='./leann_index', help='Path to store index')\n    \n    # Search command\n    search_parser = subparsers.add_parser('search', help='Search the index')\n    search_parser.add_argument('query', help='Search query')\n    search_parser.add_argument('--top-k', type=int, default=10, help='Number of results')\n    search_parser.add_argument('--grep', action='store_true', help='Use exact text matching')\n    search_parser.add_argument('--filter', help='JSON metadata filter')\n    search_parser.add_argument('--index-path', default='./leann_index', help='Path to index')\n    \n    # Stats command\n    stats_parser = subparsers.add_parser('stats', help='Get index statistics')\n    stats_parser.add_argument('--index-path', default='./leann_index', help='Path to index')\n    \n    args = parser.parse_args()\n    \n    if args.command == 'index':\n        with suppress_stdout():\n            bridge = LEANNBridge(index_path=args.index_path)\n            result = bridge.index_codebase(\n                root_path=args.path,\n                extensions=args.extensions,\n                exclude_dirs=args.exclude\n            )\n        print_result(result)\n        \n    elif args.command == 'search':\n        with suppress_stdout():\n            bridge = LEANNBridge(index_path=args.index_path)\n            metadata_filters = None\n            if args.filter:\n                try:\n                    metadata_filters = json.loads(args.filter)\n                except:\n                    pass\n            results = bridge.search(\n                query=args.query,\n                top_k=args.top_k,\n                metadata_filters=metadata_filters,\n                use_grep=args.grep\n            )\n        print_result(results)\n        \n    elif args.command == 'stats':\n        with suppress_stdout():\n            bridge = LEANNBridge(index_path=args.index_path)\n            stats = bridge.get_index_stats()\n        print_result(stats)\n        \n    else:\n        parser.print_help()\n\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"file_path": "leann_bridge.py", "file_extension": ".py", "file_name": "leann_bridge.py", "file_size": 12286, "last_modified": 1766515923.0380616}}
{"id": "4", "text": "{\n  \"version\": \"1.0\",\n  \"backend_name\": \"hnsw\",\n  \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"dimensions\": 384,\n  \"backend_kwargs\": {},\n  \"embedding_mode\": \"sentence-transformers\",\n  \"passage_sources\": [\n    {\n      \"type\": \"jsonl\",\n      \"path\": \"test_index.passages.jsonl\",\n      \"index_path\": \"test_index.passages.idx\",\n      \"path_relative\": \"test_index.passages.jsonl\",\n      \"index_path_relative\": \"test_index.passages.idx\"\n    }\n  ],\n  \"is_compact\": true,\n  \"is_pruned\": true\n}", "metadata": {"file_path": "test_index.meta.json", "file_extension": ".json", "file_name": "test_index.meta.json", "file_size": 504, "last_modified": 1766495501.808394}}
{"id": "5", "text": "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n", "metadata": {"file_path": "test_index_robust.ids.txt", "file_extension": ".txt", "file_name": "test_index_robust.ids.txt", "file_size": 38, "last_modified": 1766495544.4763658}}
{"id": "6", "text": "{\n  \"version\": \"1.0\",\n  \"backend_name\": \"hnsw\",\n  \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"dimensions\": 384,\n  \"backend_kwargs\": {},\n  \"embedding_mode\": \"sentence-transformers\",\n  \"passage_sources\": [\n    {\n      \"type\": \"jsonl\",\n      \"path\": \"test_index_clean.passages.jsonl\",\n      \"index_path\": \"test_index_clean.passages.idx\",\n      \"path_relative\": \"test_index_clean.passages.jsonl\",\n      \"index_path_relative\": \"test_index_clean.passages.idx\"\n    }\n  ],\n  \"is_compact\": true,\n  \"is_pruned\": true\n}", "metadata": {"file_path": "test_index_clean.meta.json", "file_extension": ".json", "file_name": "test_index_clean.meta.json", "file_size": 528, "last_modified": 1766495527.3361373}}
{"id": "7", "text": "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n", "metadata": {"file_path": "test_index_clean.ids.txt", "file_extension": ".txt", "file_name": "test_index_clean.ids.txt", "file_size": 38, "last_modified": 1766495526.706788}}
{"id": "8", "text": "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n", "metadata": {"file_path": "test_index.ids.txt", "file_extension": ".txt", "file_name": "test_index.ids.txt", "file_size": 38, "last_modified": 1766495501.3510575}}
{"id": "9", "text": "{\n  \"version\": \"1.0\",\n  \"backend_name\": \"hnsw\",\n  \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"dimensions\": 384,\n  \"backend_kwargs\": {},\n  \"embedding_mode\": \"sentence-transformers\",\n  \"passage_sources\": [\n    {\n      \"type\": \"jsonl\",\n      \"path\": \"test_index_robust.passages.jsonl\",\n      \"index_path\": \"test_index_robust.passages.idx\",\n      \"path_relative\": \"test_index_robust.passages.jsonl\",\n      \"index_path_relative\": \"test_index_robust.passages.idx\"\n    }\n  ],\n  \"is_compact\": true,\n  \"is_pruned\": true\n}", "metadata": {"file_path": "test_index_robust.meta.json", "file_extension": ".json", "file_name": "test_index_robust.meta.json", "file_size": 532, "last_modified": 1766495544.9304848}}
